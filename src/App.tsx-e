import SubscribedApp from "./_pages/SubscribedApp"
import { UpdateNotification } from "./components/UpdateNotification"
import {
  QueryClient,
  QueryClientProvider
} from "@tanstack/react-query"
import { useEffect, useState, useCallback, useRef } from "react"
import {
  Toast,
  ToastDescription,
  ToastProvider,
  ToastTitle,
  ToastViewport
} from "./components/ui/toast"
import { ToastContext } from "./contexts/toast"
import { WelcomeScreen } from "./components/WelcomeScreen"
import { SettingsDialog } from "./components/Settings/SettingsDialog"

// Create a React Query client
const queryClient = new QueryClient({
  defaultOptions: {
    queries: {
      staleTime: 0,
      gcTime: Infinity,
      retry: 1,
      refetchOnWindowFocus: false
    },
    mutations: {
      retry: 1
    }
  }
})

// Root component that provides the QueryClient
function App() {
  const [toastState, setToastState] = useState({
    open: false,
    title: "",
    description: "",
    variant: "neutral" as "neutral" | "success" | "error"
  })
  const [credits, setCredits] = useState<number>(999) // Unlimited credits
  const [currentLanguage, setCurrentLanguage] = useState<string>("python")
  const [isInitialized, setIsInitialized] = useState(false)
  const [hasApiKey, setHasApiKey] = useState(false)
  const [apiKeyDialogOpen, setApiKeyDialogOpen] = useState(false)
  // Note: Model selection is now handled via separate extraction/solution/debugging model settings

  const [isSettingsOpen, setIsSettingsOpen] = useState(false)
  const [isChatPanelOpen, setIsChatPanelOpen] = useState(false);
  const [chatHistory, setChatHistory] = useState<any[]>([]);
  const [chatInputValue, setChatInputValue] = useState("");

  // Voice recording state
  const [isRecording, setIsRecording] = useState(false);
  const [mediaRecorder, setMediaRecorder] = useState<MediaRecorder | null>(null);
  const [audioChunks, setAudioChunks] = useState<Blob[]>([]);
  const audioChunksRef = useRef<Blob[]>([]);

  // Show toast method
  const showToast = useCallback(
    (
      title: string,
      description: string,
      variant: "neutral" | "success" | "error"
    ) => {
      setToastState({
        open: true,
        title,
        description,
        variant
      })
    },
    []
  )

  // NEW: Whisper-based voice transcription
  const startRecording = useCallback(async () => {
    try {
      console.log("Starting recording...");
      
      // Fetch OpenAI API key
      const apiKey = await window.electronAPI.getOpenAIApiKey();
      if (!apiKey) {
        console.error("OpenAI API key not found");
        showToast("API Key Missing", "OpenAI API key is not configured", "error");
        return;
      }
      
      // Get media stream
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const mimeType = 'audio/webm';
      
      // Create and configure media recorder
      const recorder = new MediaRecorder(stream, { mimeType });
      setMediaRecorder(recorder);
      audioChunksRef.current = [];
      
      // Initialize state for showing partial results
      let transcriptionInProgress = false;
      let processingInterval: ReturnType<typeof setInterval> | null = null;
      let lastProcessedChunkIndex = -1;
      
      // Start a processing loop that sends chunks for transcription every few seconds
      processingInterval = setInterval(async () => {
        if (transcriptionInProgress || audioChunksRef.current.length <= lastProcessedChunkIndex + 1) {
          return; // Skip if already processing or no new chunks
        }
        
        const chunksToProcess = audioChunksRef.current.slice(0, lastProcessedChunkIndex + 4); // Process up to 4 chunks
        
        if (chunksToProcess.length > 0) {
          transcriptionInProgress = true;
          
          try {
            // Show processing indicator
            showToast("Processing", "Transcribing speech...", "neutral");
            
            // Create a temporary blob for this batch
            const audioBlob = new Blob(chunksToProcess, { type: mimeType });
            
            // Process the partial audio
            const result = await processPartialAudio(audioBlob, apiKey);
            
            if (result && result.text) {
              // Update chat input with transcription in progress
              setChatInputValue(prev => {
                // If starting new transcription, add placeholder
                if (!prev.includes("[Transcribing...]")) {
                  return `${prev}[Transcribing...] ${result.text}`;
                }
                
                // Otherwise update the existing placeholder
                const baseText = prev.replace(/\[Transcribing...\] .*$/, '');
                return `${baseText}[Transcribing...] ${result.text}`;
              });
            }
            
            // Update the last processed chunk index
            lastProcessedChunkIndex = chunksToProcess.length - 1;
          } catch (error) {
            console.error("Error processing partial audio:", error);
          } finally {
            transcriptionInProgress = false;
          }
        }
      }, 3000); // Process every 3 seconds
      
      // Collect audio chunks
      recorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          audioChunksRef.current.push(e.data);
        }
      };
      
      // Set up event handlers with enhanced logging
      recorder.onstart = () => {
        console.log("MediaRecorder started, state:", recorder.state);
        // Initialize the chat input with a placeholder for transcription
        setChatInputValue(prev => `${prev}[Transcribing...] `);
      };
      
      recorder.onerror = (e) => {
        console.error("MediaRecorder error:", e);
        if (processingInterval) {
          clearInterval(processingInterval);
        }
      };
      
      // Process final audio when recording stops
      recorder.onstop = () => {
        console.log("MediaRecorder stopped");
        
        // Clear the processing interval
        if (processingInterval) {
          clearInterval(processingInterval);
        }
        
        if (audioChunksRef.current.length > 0) {
          console.log("Processing complete audio with Whisper API");
          // Clean up the transcribing placeholder
          setChatInputValue(prev => prev.replace(/\[Transcribing...\] .*$/, ''));
          // Process the full audio for final result
          processAudioWithWhisper(audioChunksRef.current);
        }
      };
      
      // Start recording
      recorder.start(1000); // Record in chunks of 1 second
      setIsRecording(true);
      showToast('Recording', 'Listening... Click the mic button again to stop', 'success');

    } catch (error: unknown) {
      console.error('Recording error:', error);
      const errorMessage = error instanceof Error ? error.message : 'Failed to access microphone';
      showToast('Microphone Error', errorMessage, 'error');
    }
  }, [showToast]);
  
  // Helper function to process partial audio chunks during recording
  const processPartialAudio = async (audioBlob: Blob, apiKey: string) => {
    try {
      // Create FormData and append the file
      const formData = new FormData();
      const file = new File([audioBlob], "partial_audio.webm", { type: 'audio/webm' });
      formData.append("file", file);
      formData.append("model", "whisper-1");
      formData.append("language", "en");
      
      // Make request to Whisper API
      const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${apiKey}`
        },
        body: formData
      });
      
      if (!response.ok) {
        const errorData = await response.json().catch(() => null);
        throw new Error(errorData?.error?.message || `API returned ${response.status}: ${response.statusText}`);
      }
      
      const data = await response.json();
      console.log("Partial transcription result:", data);
      
      return data;
    } catch (error) {
      console.error("Error in partial audio processing:", error);
      return null;
    }
  };
  
  const stopRecording = useCallback(() => {
    if (mediaRecorder && mediaRecorder.state !== 'inactive') {
      mediaRecorder.stop();
      setIsRecording(false);
    }
  }, [mediaRecorder]);

  // Set unlimited credits
  const updateCredits = useCallback(() => {
    setCredits(999) // No credit limit in this version
    window.__CREDITS__ = 999
  }, [])

  // Helper function to safely update language
  const updateLanguage = useCallback((newLanguage: string) => {
    setCurrentLanguage(newLanguage)
    window.__LANGUAGE__ = newLanguage
  }, [])

  // Helper function to mark initialization complete
  const markInitialized = useCallback(() => {
    setIsInitialized(true)
    window.__IS_INITIALIZED__ = true
  }, [])

  // Check for OpenAI API key and prompt if not found
  useEffect(() => {
    const checkApiKey = async () => {
      try {
        const hasKey = await window.electronAPI.checkApiKey()
        setHasApiKey(hasKey)
        
        // If no API key is found, show the settings dialog after a short delay
        if (!hasKey) {
          setTimeout(() => {
            setIsSettingsOpen(true)
          }, 1000)
        }
      } catch (error) {
        console.error("Failed to check API key:", error)
      }
    }
    
    if (isInitialized) {
      checkApiKey()
    }
  }, [isInitialized])

  // Initialize dropdown handler
  useEffect(() => {
    if (isInitialized) {
      // Process all types of dropdown elements with a shorter delay
      const timer = setTimeout(() => {
        // Find both native select elements and custom dropdowns
        const selectElements = document.querySelectorAll('select');
        const customDropdowns = document.querySelectorAll('.dropdown-trigger, [role="combobox"], button:has(.dropdown)');
        
        // Enable native selects
        selectElements.forEach(dropdown => {
          dropdown.disabled = false;
        });
        
        // Enable custom dropdowns by removing any disabled attributes
        customDropdowns.forEach(dropdown => {
          if (dropdown instanceof HTMLElement) {
            dropdown.removeAttribute('disabled');
            dropdown.setAttribute('aria-disabled', 'false');
          }
        });
        
        console.log(`Enabled ${selectElements.length} select elements and ${customDropdowns.length} custom dropdowns`);
      }, 1000);
      
      return () => clearTimeout(timer);
    }
  }, [isInitialized]);

  // Listen for settings dialog open requests
  useEffect(() => {
    const unsubscribeSettings = window.electronAPI.onShowSettings(() => {
      console.log("Show settings dialog requested");
      setIsSettingsOpen(true);
    });
    
    return () => {
      unsubscribeSettings();
    };
  }, []);

  // Initialize basic app state
  useEffect(() => {
    // Load config and set values
    const initializeApp = async () => {
      try {
        // Set unlimited credits
        updateCredits()
        
        // Load config including language and model settings
        const config = await window.electronAPI.getConfig()
        
        // Load language preference
        if (config && config.language) {
          updateLanguage(config.language)
        } else {
          updateLanguage("python")
        }
        
        // Model settings are now managed through the settings dialog
        // and stored in config as extractionModel, solutionModel, and debuggingModel
        
        markInitialized()
      } catch (error) {
        console.error("Failed to initialize app:", error)
        // Fallback to defaults
        updateLanguage("python")
        markInitialized()
      }
    }
    
    initializeApp()

    // Event listeners for process events
    const onApiKeyInvalid = () => {
      showToast(
        "API Key Invalid",
        "Your OpenAI API key appears to be invalid or has insufficient credits",
        "error"
      )
      setApiKeyDialogOpen(true)
    }

    // Setup API key invalid listener
    window.electronAPI.onApiKeyInvalid(onApiKeyInvalid)

    // Define a no-op handler for solution success
    const unsubscribeSolutionSuccess = window.electronAPI.onSolutionSuccess(
      () => {
        console.log("Solution success - no credits deducted in this version")
        // No credit deduction in this version
      }
    )

    // Cleanup function
    return () => {
      window.electronAPI.removeListener("API_KEY_INVALID", onApiKeyInvalid)
      unsubscribeSolutionSuccess()
      window.__IS_INITIALIZED__ = false
      setIsInitialized(false)
    }
  }, [updateCredits, updateLanguage, markInitialized, showToast])

  // API Key dialog management
  const handleOpenSettings = useCallback(() => {
    console.log('Opening settings dialog');
    setIsSettingsOpen(true);
  }, []);
  
  const handleCloseSettings = useCallback((open: boolean) => {
    console.log('Settings dialog state changed:', open);
    setIsSettingsOpen(open);
  }, []);

  const handleApiKeySave = useCallback(async (apiKey: string) => {
    try {
      await window.electronAPI.updateConfig({ apiKey })
      setHasApiKey(true)
      showToast("Success", "API key saved successfully", "success")
      
      // Reload app after a short delay to reinitialize with the new API key
      setTimeout(() => {
        window.location.reload()
      }, 1500)
    } catch (error) {
      console.error("Failed to save API key:", error)
      showToast("Error", "Failed to save API key", "error")
    }
  }, [showToast])

  // Chat panel toggle
  const toggleChatPanel = useCallback(() => {
    setIsChatPanelOpen(prev => !prev);
  }, []);

  // NEW: Direct Web Speech API implementation - MOVED ABOVE onToggleVoiceInput
  const toggleWebSpeech = useCallback(() => {
    // Disable speech recognition but keep the interface working
    showToast("Info", "Voice recognition is currently disabled in this version", "neutral");
    console.log("Voice recognition is disabled in this version");
    return;
  }, [showToast]);
  
  // --- Update onToggleVoiceInput to use the new recording functions --- 
  const onToggleVoiceInput = useCallback(() => {
    if (isRecording) {
      stopRecording();
    } else {
      startRecording();
    }
  }, [isRecording, startRecording, stopRecording]);

  // Chat message handler
  const handleSendMessage = async (messageContent: string) => {
    if (!messageContent.trim()) return;
        
    const newUserMessage = { role: 'user', content: messageContent };
    setChatHistory(prev => [...prev, newUserMessage]);
    setChatInputValue(""); // Clear input immediately

    try {
      // Call the backend API for AI response
      const result = await window.electronAPI.handleAiQuery({ 
        query: messageContent, 
        language: currentLanguage 
      });

      if (result && result.success && typeof result.data === 'string') {
        const newAssistantMessage = { role: 'assistant', content: result.data };
        setChatHistory(prev => [...prev, newAssistantMessage]);
      } else {
        showToast("AI Query Error", `Error: ${result?.error || "Failed to get response."}`, "error");
        const errorMessage = { role: 'assistant', content: `Error: ${result?.error || "Failed to get response."}` };
        setChatHistory(prev => [...prev, errorMessage]);
      }
    } catch (error: any) {
      showToast("Communication Error", `Error: ${error.message || "Could not reach AI service."}`, "error");
      const errorMessage = { role: 'assistant', content: `Error: ${error.message || "Could not reach AI service."}` };
      setChatHistory(prev => [...prev, errorMessage]);
    }
  };

  // Function to process audio with Whisper REST API
  const processAudioWithWhisper = async (audioChunks: Blob[]) => {
    try {
      // Create a blob from all audio chunks
      const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
      console.log(`Created audio blob: size=${audioBlob.size} bytes, type=${audioBlob.type}`);

      // Get OpenAI API key
      const apiKey = await window.electronAPI.getOpenAIApiKey();
      if (!apiKey) {
        console.error("OpenAI API key not found");
        showToast("API Key Missing", "OpenAI API key is not configured", "error");
        return;
      }

      // Create FormData and append the file
      const formData = new FormData();
      const file = new File([audioBlob], "audio.webm", { type: 'audio/webm' });
      formData.append("file", file);
      formData.append("model", "whisper-1");
      formData.append("language", "en");

      // Show transcribing toast
      showToast("Transcribing", "Converting your speech to text...", "neutral");

      // Make request to Whisper API
      const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${apiKey}`
        },
        body: formData
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => null);
        throw new Error(errorData?.error?.message || `API returned ${response.status}: ${response.statusText}`);
      }

      const data = await response.json();
      console.log("Transcription result:", data);

      if (data.text) {
        // Update chat input with transcription
        setChatInputValue(prev => prev + (prev ? ' ' : '') + data.text);
        showToast("Transcription Complete", "Your speech has been converted to text", "success");
      } else {
        throw new Error("No transcription returned");
      }
    } catch (error) {
      console.error("Error in Whisper API processing:", error);
      showToast("Transcription Failed", error instanceof Error ? error.message : "Failed to transcribe audio", "error");
    }
  };

  return (
    <QueryClientProvider client={queryClient}>
      <ToastProvider>
        <ToastContext.Provider value={{ showToast }}>
          <div className="relative">
            {isInitialized ? (
              hasApiKey ? (
                <SubscribedApp
                  credits={credits}
                  currentLanguage={currentLanguage}
                  setLanguage={updateLanguage}
                  isMicActive={false}
                  onToggleVoice={onToggleVoiceInput}
                  onToggleChat={toggleChatPanel}
                />
              ) : (
                <WelcomeScreen onOpenSettings={handleOpenSettings} />
              )
            ) : (
              <div className="min-h-screen bg-black flex items-center justify-center">
                <div className="flex flex-col items-center gap-3">
                  <div className="w-6 h-6 border-2 border-white/20 border-t-white/80 rounded-full animate-spin"></div>
                  <p className="text-white/60 text-sm">
                    Initializing...
                  </p>
                </div>
              </div>
            )}
            <UpdateNotification />
          </div>
          
          {/* Settings Dialog */}
          <SettingsDialog 
            open={isSettingsOpen} 
            onOpenChange={handleCloseSettings} 
          />
          
          <Toast
            open={toastState.open}
            onOpenChange={(open) =>
              setToastState((prev) => ({ ...prev, open }))
            }
            variant={toastState.variant}
            duration={1500}
          >
            <ToastTitle>{toastState.title}</ToastTitle>
            <ToastDescription>{toastState.description}</ToastDescription>
          </Toast>
          <ToastViewport />
          
          {isChatPanelOpen && (
            <div className="fixed top-20 right-4 bottom-20 z-50 w-80 bg-white dark:bg-gray-800 shadow-2xl rounded-xl overflow-hidden flex flex-col border border-indigo-100 dark:border-gray-700">
              <div className="p-3 bg-gradient-to-r from-violet-600 to-indigo-700 flex justify-between items-center">
                <h3 className="font-medium text-white flex items-center text-sm">
                  <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" className="mr-1">
                    <path d="M12 2L15.09 8.26L22 9.27L17 14.14L18.18 21.02L12 17.77L5.82 21.02L7 14.14L2 9.27L8.91 8.26L12 2Z" fill="white"/>
                  </svg>
                  AI Assistant
                </h3>
                <button onClick={toggleChatPanel} className="text-white hover:text-gray-200 transition-colors">
                  <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <path d="M18 6L6 18M6 6L18 18" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"/>
                  </svg>
                </button>
              </div>
              
              <div className="flex-1 overflow-y-auto p-3 space-y-3 bg-gray-50 dark:bg-gray-900">
                {chatHistory.length === 0 && (
                  <div className="flex items-center justify-center h-full opacity-60">
                    <div className="text-center">
                      <svg width="32" height="32" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" className="mx-auto mb-2 text-indigo-500">
                        <path d="M12 2L15.09 8.26L22 9.27L17 14.14L18.18 21.02L12 17.77L5.82 21.02L7 14.14L2 9.27L8.91 8.26L12 2Z" fill="currentColor"/>
                      </svg>
                      <p className="text-gray-500 dark:text-gray-400 text-sm">Ask a question</p>
                    </div>
                  </div>
                )}
                {chatHistory.map((msg, i) => (
                  <div key={i} className={`flex ${msg.role === 'user' ? 'justify-end' : 'justify-start'}`}>
                    <div className={`max-w-[85%] p-2 rounded-lg text-sm ${
                      msg.role === 'user' 
                        ? 'bg-indigo-600 text-white rounded-tr-none shadow-sm' 
                        : 'bg-white dark:bg-gray-800 text-black dark:text-white rounded-tl-none shadow-md border border-gray-100 dark:border-gray-700'
                    }`}>
                      {msg.content}
                    </div>
                  </div>
                ))}
              </div>
              
              <div className="p-3 border-t border-gray-200 dark:border-gray-700 bg-white dark:bg-gray-800">
                <form 
                  onSubmit={(e) => {
                    e.preventDefault();
                    if (chatInputValue.trim()) {
                      handleSendMessage(chatInputValue);
                    }
                  }}
                  className="flex space-x-2"
                >
                  <input
                    type="text"
                    value={chatInputValue}
                    onChange={(e) => setChatInputValue(e.target.value)}
                    className="flex-1 p-2 border border-gray-300 dark:border-gray-600 rounded-lg bg-white dark:bg-gray-700 text-black dark:text-white focus:outline-none focus:ring-2 focus:ring-indigo-500 focus:border-transparent text-sm"
                    placeholder="Type your message..."
                  />
                  <button
                    type="submit"
                    disabled={!chatInputValue.trim()}
                    className="p-2 bg-gradient-to-r from-violet-600 to-indigo-700 hover:from-violet-500 hover:to-indigo-600 text-white rounded-lg disabled:opacity-50 transition-colors"
                  >
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                      <path d="M22 2L11 13" stroke="white" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"/>
                      <path d="M22 2L15 22L11 13L2 9L22 2Z" stroke="white" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"/>
                    </svg>
                  </button>
                </form>
              </div>
            </div>
          )}
        </ToastContext.Provider>
      </ToastProvider>
    </QueryClientProvider>
  )
}

export default App